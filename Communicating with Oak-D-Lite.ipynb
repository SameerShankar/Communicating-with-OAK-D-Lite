{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0035dfff-8ef7-450a-98dd-2af1dc745383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import depthai as dai\n",
    "pipeline = dai.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b085f22-b35b-4fb7-9a64-30d3acfa4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a mono camera is one of the left or right ones (greyscale depth perception ones). We have just created a node for the mono camera\n",
    "\n",
    "mono = pipeline.createMonoCamera()\n",
    "\n",
    "#specify that it is the left camera. Creates XLinkIn internally, connecting the host (the computer) TO THE device (the OAK D).\n",
    "\n",
    "mono.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "\n",
    "#Creates XLinkOut, and names it specifically for the given camera (can have XLinkOut's for multiple cameras).\n",
    "xout = pipeline.createXLinkOut()\n",
    "xout.setStreamName(\"left\")\n",
    "#The below links the mono camera to the actual output. Takes the output of the mono camera and putting it as the input to the XLinkOut. We are attaching the camera (left camera) to xLinkOut (left)\n",
    "mono.out.link(xout.input)\n",
    "\n",
    "#transfers all the code from the host to the actual device\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "    \n",
    "    # queries the XLinkOut from the device to the host\n",
    "    queue = device.getOutputQueue(name = \"left\")\n",
    "    #gets frame from device\n",
    "    frame = queue.get()\n",
    "    \n",
    "    #getCvFrame() converts frame into the OpenCV format\n",
    "    imOut = frame.getCvFrame()\n",
    "    #displays frame\n",
    "    cv2.imshow(\"Image\", imOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc460d6f-8872-461e-a99f-91400ae50c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rectifiedLeft, rectifiedRight and disparity are XLinkOut nodes\n",
    "\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "\n",
    "def getFrame(queue): #gets the last frame from the output queue, and converts to OpenCV format\n",
    "    # Get frame from queue\n",
    "    frame = queue.get()\n",
    "    # Convert frame to OpenCV format and return\n",
    "    return frame.getCvFrame()\n",
    "\n",
    "def getMonoCamera(pipeline, isLeft):\n",
    "    # Configure mono camera\n",
    "    mono = pipeline.createMonoCamera()\n",
    "    \n",
    "    # Set camera resolution\n",
    "    mono.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "    \n",
    "    if isLeft:\n",
    "        # Get Left camera\n",
    "        mono.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "    else:\n",
    "        # Get Right camera\n",
    "        mono.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "    return mono\n",
    "\n",
    "# NEW CODE\n",
    "\n",
    "def getStereoPair(pipeline, monoLeft, monoRight):\n",
    "    # Configure stereo pair for depth estimation\n",
    "    stereo = pipeline.createStereoDepth() #creating the stereo depth node\n",
    "    # Checks occluded (when object comes too close to the device) pixels and marks them invalid\n",
    "    stereo.setLeftRightCheck(True)\n",
    "    \n",
    "    # Configure left and right cameras to work as a stereo pair\n",
    "    monoLeft.out.link(stereo.left)\n",
    "    monoRight.out.link(stereo.right)\n",
    "    \n",
    "    return stereo\n",
    "\n",
    "def mouseCallback(event, x, y, flags, param):\n",
    "    global mouseX, mouseY\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        mouseX = x\n",
    "        mouseY = y\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mouseX = 0\n",
    "    mouseY = 640\n",
    "    \n",
    "    # Start defining a pipeline\n",
    "    pipeline = dai.Pipeline()\n",
    "    \n",
    "    # Set up left and right cameras\n",
    "    monoLeft = getMonoCamera(pipeline, isLeft = True)\n",
    "    monoRight = getMonoCamera(pipeline, isLeft = False)\n",
    "    \n",
    "    # Combine left and right cameras to form a stereo pair\n",
    "    stereo = getStereoPair(pipeline, monoLeft, monoRight) # taking the output from the left and right mono cameras and putting them as input to the stereo camera\n",
    "    \n",
    "    # Define and name output depth map, \n",
    "    # xoutDepth = pipeline.createXLinkOut()\n",
    "    # xoutDepth.setStreamName(\"depth\")\n",
    "    \n",
    "    # Set XLinkOut for disparity, rectifiedLeft, rectifiedRight\n",
    "    \n",
    "    xoutDisp = pipeline.createXLinkOut()\n",
    "    xoutDisp.setStreamName(\"disparity\")\n",
    "    \n",
    "    xoutRectifiedLeft = pipeline.createXLinkOut()\n",
    "    xoutRectifiedLeft.setStreamName(\"rectifiedLeft\")\n",
    "    \n",
    "    xoutRectifiedRight = pipeline.createXLinkOut()\n",
    "    xoutRectifiedRight.setStreamName(\"rectifiedRight\")\n",
    "    \n",
    "    # Pipeline is defined, now we can connect to the device\n",
    "    \n",
    "    with dai.Device(pipeline) as device:\n",
    "        \n",
    "        # Output queues will be used to get the rgb frames and nn data from the outputs defined above. Gets the frames back.\n",
    "        \n",
    "        disparityQueue = device.getOutputQueue(name = 'disparity', maxSize = 1, blocking = False)\n",
    "        rectifiedLeftQueue = device.getOutputQueue(name = 'rectifiedLeft', maxSize = 1, blocking = False)\n",
    "        rectifiedRightQueue = device.getOutputQueue(name = 'rectifiedRight', maxSize = 1, blocking = False)\n",
    "        \n",
    "        # Calculate a multiplier for colormapping disparity map\n",
    "        disparityMultiplier = 255 / stereo.getMaxDisparity()\n",
    "        \n",
    "        cv2.namedWindow(\"Stereo Pair\")\n",
    "        cv2.setMouseCallback(\"Stereo Pair\", mouseCallback)\n",
    "        \n",
    "        # Variable use to toggle between side by side view and one frame view.\n",
    "        sideBySide = False\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # Get disparity map\n",
    "            disparity = getFrame(disparityQueue)\n",
    "            \n",
    "            # Colormap disparity for display\n",
    "            disparity = (disparity * disparityMultiplier).astype(np.uint8)\n",
    "            disparity = cv2.applyColorMap(disparity, cv2.COLORMAP_JET)\n",
    "            \n",
    "            # Get left and right rectified frame\n",
    "            leftFrame = getFrame(rectifiedLeftQueue);\n",
    "            rightFrame = getFrame(rectifiedLeftQueue)\n",
    "            \n",
    "        if sideBySide:\n",
    "            # Show side by side view\n",
    "            imOut = np.hstack((leftFrame, rightFrame))\n",
    "        else:\n",
    "            #Show overlapping frames\n",
    "            imOut = np.uint8(leftFrame/2 + rightFrame/2)\n",
    "        \n",
    "        #converting image to colour\n",
    "        imOut = cv2.cvtColor(imOut, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        imOut = cv2.line(imOut, (mouseX, mouseY), (1280, mouseY), (0, 0, 255), 2)\n",
    "        imOut = cv2.circle(imOut, (mouseX, mouseY), 2, (255, 255, 128), 2)\n",
    "        cv2.imshow(\"Stereo Pair\", imOut)\n",
    "        cv2.imshow(\"Disparity\", imOut)\n",
    "        \n",
    "        # Check for keyboard input\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            # Quit when q is pressed\n",
    "            break\n",
    "        elif key == ord('t'):\n",
    "            # Toggle display when t is pressed\n",
    "            sideBySide = not sideBySide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795743b0-0786-46f3-b35d-6a9597bec5c7",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "- Depth is inversely proportional to disparity; the larger the disparity (the same image being apart due to left and right view) is, the closer that image would then be (lower depth)\n",
    "- Shadowish areas are where both cameras cannot see (depth is incorrect as both cameras need to be able to see the point in order to estimate depth)\n",
    "- Another reason there could be a shadow-like area (incorrect depth perception) is that usually there are two pixels at different points (seen by the different cameras), that are usually matched. If you cannot do that, depth perception will go wrong. E.g. A flat white wall will have identical pixels all around, therefore even though we may be able to see it, the two cameras won't be able to match the points because they all look the same. Even shiny surfaces face this problem.\n",
    "- There is a minimum distance from the camera that the object/person must be farther than to get depth perception. If we are too close, the disparity will be very large, and Oak D is not programmed to look for such a large disparity (for efficiency purposes). This can be altered (make the search space larger), but will make the depth estimation slower. (Too close and the depth estimate will be poor as we are not searching in that disparity range)\n",
    "- Scan lines will show that a point will lie on the same vertical axis (if we slice horizontally) because of image rectification.\n",
    "- To find corresponding points on two different mono cameras, you would choose a point on one camera, and then move along the scan line on the other camera, but not look through the entire scan line (otherwise it would take too long), you just need a small number of points to check over."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
